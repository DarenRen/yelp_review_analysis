{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.style.use('seaborn')\n",
    "!source activate py36;pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating Distribution Pie Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Predict_SVM.csv') # the prediction results using SVM\n",
    "data = data[['rating', 'predict_SVM']]\n",
    "data['predict_SVM'] = np.where((data['predict_SVM'] == 1), True, False)\n",
    "data.reset_index(level = 0, inplace = True)\n",
    "d = data.groupby(['rating'])['predict_SVM'].value_counts().unstack().plot.pie(subplots = True, autopct = '%.2f%%', figsize = (10,4.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews Text Length Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Predict_SVM.csv')\n",
    "\n",
    "def fix_review(input_review): # fix review content\n",
    "    output_review = []\n",
    "    input_review = re.sub(r'[^\\w\\s]', '', input_review).replace('\\xa0', '').replace('\\n\\n', ' ').replace('\\n', ' ').strip().lower().split(' ')\n",
    "    for i in input_review:\n",
    "        if i != '':\n",
    "            output_review.append(i)\n",
    "    return output_review\n",
    "df['review'] = df['review'].apply(fix_review)\n",
    "\n",
    "df['text_length'] = 0\n",
    "for i in range(len(df)):\n",
    "    df['text_length'].loc[i] = len(df['review'].loc[i])\n",
    "    \n",
    "df_true = df[df['predict_SVM'] == 1]\n",
    "df_fake = df[df['predict_SVM'] == 0]\n",
    "\n",
    "bins = [] # set histogram bins\n",
    "for i in range(1,40):\n",
    "    bins.append(i)\n",
    "\n",
    "length_rating = df_true.groupby(['text_length', 'rating']).size().unstack()\n",
    "COL_NUM = 5\n",
    "ROW_NUM = 1\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(25,5))\n",
    "# fig.suptitle('True Review Text Length Distribution')\n",
    "for i, (rating, text_length) in enumerate(length_rating.items()): \n",
    "    ax = axes[i]\n",
    "    text_length.plot.hist(grid=True, bins=bins, rwidth=1, ax=ax)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    ax.set_title(f\"Stars: {rating}\")\n",
    "    ax.set_ylim([0, 130])    \n",
    "plt.tight_layout() \n",
    "\n",
    "length_rating = df_fake.groupby(['text_length', 'rating']).size().unstack()\n",
    "COL_NUM = 5\n",
    "ROW_NUM = 1\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(25,5))\n",
    "# fig.suptitle('Fake Review Text Length Distribution')\n",
    "for i, (rating, text_length) in enumerate(length_rating.items()): \n",
    "    ax = axes[i]\n",
    "    text_length.plot.hist(grid=True, bins=bins, rwidth=1, ax=ax)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    ax.set_title(f\"Stars: {rating}\")\n",
    "    ax.set_ylim([0, 130])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Text Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_text = ''\n",
    "for i in range(len(df_true)):\n",
    "    true_text += ' '.join(df_true['review'].iloc[i])\n",
    "fake_text = ''\n",
    "for i in range(len(df_fake)):\n",
    "    fake_text += ' '.join(df_fake['review'].iloc[i])\n",
    "true_string = true_text.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "fake_string = fake_text.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=800, max_words=40).generate(true_string)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=800,max_words=40).generate(fake_string)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_comparison(texts):\n",
    "    headers = ['pos','neg','neu','compound']\n",
    "    print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in range(len(texts)):\n",
    "        name = texts[i][0]\n",
    "        sentences = sent_tokenize(texts[i][1])\n",
    "        pos=compound=neu=neg=0\n",
    "        for sentence in sentences:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            pos+=vs['pos']/(len(sentences))\n",
    "            compound+=vs['compound']/(len(sentences))\n",
    "            neu+=vs['neu']/(len(sentences))\n",
    "            neg+=vs['neg']/(len(sentences))\n",
    "        print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)\n",
    "\n",
    "df1 = pd.read_csv('Predict_SVM.csv')\n",
    "df1_true = df1[df1['predict_SVM'] == 1]\n",
    "df1_fake = df1[df1['predict_SVM'] == 0]\n",
    "\n",
    "true_text = ''\n",
    "for i in df1_true['review']:\n",
    "    true_text += i.strip().replace('\\n\\n', '').replace('\\n', '').replace(\"\\\\\", '')\n",
    "fake_text = ''\n",
    "for i in df1_fake['review']:\n",
    "    fake_text += i.strip().replace('\\n\\n', '').replace('\\n', '').replace(\"\\\\\", '')\n",
    "texts = [('true', true_text), ('fake', fake_text)]\n",
    "\n",
    "vader_comparison(texts)\n",
    "\n",
    "# show the results using figure\n",
    "x = ['pos', 'neg', 'neu', 'compound']\n",
    "y1 = [0.20, 0.03, 0.77, 0.34]\n",
    "y2 = [0.16, 0.08, 0.76, 0.14]\n",
    "plt.title('Weighted Sentiment Analysis')\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, y1, color='blue', label = 'true reviews')\n",
    "plt.plot(x, y2, color='red', label = 'fake reviews')\n",
    "plt.legend()\n",
    "plt.xlabel('sentiments')\n",
    "plt.ylabel('weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
