{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pylab \n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)\n",
    "predict_data = pd.read_csv('scraping_data.csv') # the data scraped from website, used for prediction\n",
    "data = pd.read_csv('merged_data.csv') # the data we got from professor ShebutiÂ Rayana, used for learning\n",
    "data = data[['review', 'label']]\n",
    "data['label'] = np.where((data['label'] == 1), 1, 0)\n",
    "positive = data[data['label'] == 1]\n",
    "negative = data[data['label'] == 0]\n",
    "\n",
    "# since the data is unbalanced, the number of true reviews is around 10 times of the false reviews\n",
    "# so we subset the volume of the positive data\n",
    "positive = positive.sample(frac = 0.16)\n",
    "Corpus = positive.append(negative)\n",
    "Corpus = Corpus.reset_index(drop = True)\n",
    "\n",
    "# word tokenize\n",
    "Corpus['review'].dropna(inplace = True)\n",
    "Corpus['review'] = [entry.lower() for entry in Corpus['review']]\n",
    "Corpus['review'] = [word_tokenize(entry) for entry in Corpus['review']]\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Corpus['review']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    Corpus.loc[index, 'text_final'] = str(Final_words)\n",
    "\n",
    "# split datasets\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'], Corpus['label'], test_size=0.3)\n",
    "\n",
    "# tfidf classification of review test\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "Tfidf_vect = TfidfVectorizer(max_features = 5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(Train_X_Tfidf, Train_Y)\n",
    "predictions_linear  = model.predict(Test_X_Tfidf)\n",
    "\n",
    "# prediction using linear regression\n",
    "predict_linear = list()\n",
    "for i in range(0, len(predict_data['review'])):\n",
    "    predict_value = model.predict(Tfidf_vect.transform([str(predict_data['review'][i])]))\n",
    "    predict_data['predict_Linear'][i] = predict_value\n",
    "    \n",
    "# prediction performance\n",
    "def c_m_analysis(true, pred, threshold):\n",
    "    tn, fp, fn, tp = confusion_matrix(true, get_classification(pred,threshold)).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    tpr = tp / (tp + fn)\n",
    "    f_score = 2 * precision * tpr / (precision + tpr)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print(\"Precision:\\t\\t\\t%1.2f\" % (precision))\n",
    "    print(\"Recall/TPR:\\t\\t\\t%1.2f\" % (recall))\n",
    "    print(\"f-score:\\t\\t\\t%1.2f\" % (f_score))\n",
    "    print(\"Accuracy:\\t\\t\\t%1.2f\" % (accuracy))\n",
    "c_m_analysis(Test_Y, predictions_linear, 0.5)\n",
    "\n",
    "# confusion matrix\n",
    "def get_classification(predictions,threshold):  #take value of prediction -> 0, 1\n",
    "    classes = np.zeros_like(predictions_linear)\n",
    "    for i in range(len(classes)):\n",
    "        if predictions[i] > threshold:\n",
    "            classes[i] = 1\n",
    "    return classes\n",
    "confusion_matrix(Test_Y, get_classification(predictions_linear, 0.5))\n",
    "\n",
    "# draw roc curve\n",
    "(fpr, tpr, thresholds) = roc_curve(Test_Y, predictions_linear)\n",
    "area = auc(fpr, tpr)\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, label = \"Out-Sample ROC Curve with area = %1.2f\" % area)\n",
    "plt.plot([0, 1], [0, 1], 'k')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = tree.DecisionTreeClassifier(max_depth = 10, criterion = 'entropy')\n",
    "tree_model.fit(Train_X_Tfidf, Train_Y,Train_Y)\n",
    "predictions_tree = tree_model.predict(Test_X_Tfidf) # prediction\n",
    "confusion_matrix(Test_Y, get_classification(predictions_tree, 0.5)) # confusion matrix\n",
    "c_m_analysis(Test_Y, predictions_tree, 0.5) # prediction performance\n",
    "# roc curve is as the same codes as above, the only difference is following:\n",
    "(fpr, tpr, thresholds1) = roc_curve(Test_Y, predictions_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf, Train_Y)\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf) # prediction\n",
    "confusion_matrix(Test_Y, get_classification(predictions_NB, 0.5)) # confusion matrix\n",
    "c_m_analysis(Test_Y, predictions_NB, 0.5) # prediction performance\n",
    "# roc curve is as the same codes as above, the only difference is following:\n",
    "(fpr, tpr, thresholds) = roc_curve(Test_Y, predictions_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C = 1.0, kernel = 'linear', degree = 3, gamma = 'auto')\n",
    "SVM.fit(Train_X_Tfidf, Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf) # prediction\n",
    "np.set_printoptions(threshold = np.inf)\n",
    "confusion_matrix(Test_Y,get_classification(predictions_SVM,0.5)) # confusion matrix\n",
    "c_m_analysis(Test_Y, predictions_SVM, 0.5) # prediction performance\n",
    "# roc curve is as the same codes as above, the only difference is following:\n",
    "(fpr, tpr, thresholds) = roc_curve(Test_Y, predictions_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating Distribution Pie Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Predict_SVM.csv') # the prediction results using SVM\n",
    "data = data[['rating', 'predict_SVM']]\n",
    "data['predict_SVM'] = np.where((data['predict_SVM'] == 1), True, False)\n",
    "data.reset_index(level = 0, inplace = True)\n",
    "d = data.groupby(['rating'])['predict_SVM'].value_counts().unstack().plot.pie(subplots = True, autopct = '%.2f%%', figsize = (10,4.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews Text Length Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Predict_SVM.csv')\n",
    "\n",
    "def fix_review(input_review): # fix review content\n",
    "    output_review = []\n",
    "    input_review = re.sub(r'[^\\w\\s]', '', input_review).replace('\\xa0', '').replace('\\n\\n', ' ').replace('\\n', ' ').strip().lower().split(' ')\n",
    "    for i in input_review:\n",
    "        if i != '':\n",
    "            output_review.append(i)\n",
    "    return output_review\n",
    "df['review'] = df['review'].apply(fix_review)\n",
    "\n",
    "df['text_length'] = 0\n",
    "for i in range(len(df)):\n",
    "    df['text_length'].loc[i] = len(df['review'].loc[i])\n",
    "    \n",
    "df_true = df[df['predict_SVM'] == 1]\n",
    "df_fake = df[df['predict_SVM'] == 0]\n",
    "\n",
    "bins = [] # set histogram bins\n",
    "for i in range(1,40):\n",
    "    bins.append(i)\n",
    "\n",
    "length_rating = df_true.groupby(['text_length', 'rating']).size().unstack()\n",
    "COL_NUM = 5\n",
    "ROW_NUM = 1\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(25,5))\n",
    "# fig.suptitle('True Review Text Length Distribution')\n",
    "for i, (rating, text_length) in enumerate(length_rating.items()): \n",
    "    ax = axes[i]\n",
    "    text_length.plot.hist(grid=True, bins=bins, rwidth=1, ax=ax)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    ax.set_title(f\"Stars: {rating}\")\n",
    "    ax.set_ylim([0, 130])    \n",
    "plt.tight_layout() \n",
    "\n",
    "length_rating = df_fake.groupby(['text_length', 'rating']).size().unstack()\n",
    "COL_NUM = 5\n",
    "ROW_NUM = 1\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(25,5))\n",
    "# fig.suptitle('Fake Review Text Length Distribution')\n",
    "for i, (rating, text_length) in enumerate(length_rating.items()): \n",
    "    ax = axes[i]\n",
    "    text_length.plot.hist(grid=True, bins=bins, rwidth=1, ax=ax)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    ax.set_title(f\"Stars: {rating}\")\n",
    "    ax.set_ylim([0, 130])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Text Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_text = ''\n",
    "for i in range(len(df_true)):\n",
    "    true_text += ' '.join(df_true['review'].iloc[i])\n",
    "fake_text = ''\n",
    "for i in range(len(df_fake)):\n",
    "    fake_text += ' '.join(df_fake['review'].iloc[i])\n",
    "true_string = true_text.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "fake_string = fake_text.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=800, max_words=40).generate(true_string)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=800,max_words=40).generate(fake_string)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_comparison(texts):\n",
    "    headers = ['pos','neg','neu','compound']\n",
    "    print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in range(len(texts)):\n",
    "        name = texts[i][0]\n",
    "        sentences = sent_tokenize(texts[i][1])\n",
    "        pos=compound=neu=neg=0\n",
    "        for sentence in sentences:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            pos+=vs['pos']/(len(sentences))\n",
    "            compound+=vs['compound']/(len(sentences))\n",
    "            neu+=vs['neu']/(len(sentences))\n",
    "            neg+=vs['neg']/(len(sentences))\n",
    "        print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)\n",
    "\n",
    "df1 = pd.read_csv('Predict_SVM.csv')\n",
    "df1_true = df1[df1['predict_SVM'] == 1]\n",
    "df1_fake = df1[df1['predict_SVM'] == 0]\n",
    "\n",
    "true_text = ''\n",
    "for i in df1_true['review']:\n",
    "    true_text += i.strip().replace('\\n\\n', '').replace('\\n', '').replace(\"\\\\\", '')\n",
    "fake_text = ''\n",
    "for i in df1_fake['review']:\n",
    "    fake_text += i.strip().replace('\\n\\n', '').replace('\\n', '').replace(\"\\\\\", '')\n",
    "texts = [('true', true_text), ('fake', fake_text)]\n",
    "\n",
    "vader_comparison(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['pos', 'neg', 'neu', 'compound']\n",
    "y1 = [0.20, 0.03, 0.77, 0.34]\n",
    "y2 = [0.16, 0.08, 0.76, 0.14]\n",
    "# plt.title('Weighted Sentiment Analysis')\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, y1, color='blue', label = 'true reviews')\n",
    "plt.plot(x, y2, color='red', label = 'fake reviews')\n",
    "plt.legend()\n",
    "plt.xlabel('sentiments')\n",
    "plt.ylabel('weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
