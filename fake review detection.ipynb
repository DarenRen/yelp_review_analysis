{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pylab \n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)\n",
    "predict_data = pd.read_csv('scraping_data.csv') # the data scraped from website, used for prediction\n",
    "data = pd.read_csv('merged_data.csv') # the data we got from , used for learning\n",
    "data = data[['review', 'label']]\n",
    "data['label'] = np.where((data['label'] == 1), 1, 0)\n",
    "positive = data[data['label'] == 1]\n",
    "negative = data[data['label'] == 0]\n",
    "\n",
    "# since the data is unbalanced, the number of true reviews is around 10 times of the false reviews\n",
    "# so we subset the volume of the positive data\n",
    "positive = positive.sample(frac = 0.16)\n",
    "Corpus = positive.append(negative)\n",
    "Corpus = Corpus.reset_index(drop = True)\n",
    "\n",
    "# word tokenize\n",
    "Corpus['review'].dropna(inplace = True)\n",
    "Corpus['review'] = [entry.lower() for entry in Corpus['review']]\n",
    "Corpus['review'] = [word_tokenize(entry) for entry in Corpus['review']]\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Corpus['review']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    Corpus.loc[index, 'text_final'] = str(Final_words)\n",
    "\n",
    "# split datasets\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'], Corpus['label'], test_size=0.3)\n",
    "\n",
    "# tfidf classification of review test\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "Tfidf_vect = TfidfVectorizer(max_features = 5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(Train_X_Tfidf, Train_Y)\n",
    "predictions_linear  = model.predict(Test_X_Tfidf)\n",
    "\n",
    "# prediction using linear regression\n",
    "predict_linear = list()\n",
    "for i in range(0, len(predict_data['review'])):\n",
    "    predict_value = model.predict(Tfidf_vect.transform([str(predict_data['review'][i])]))\n",
    "    predict_data['predict_Linear'][i] = predict_value\n",
    "    \n",
    "# prediction performance\n",
    "def c_m_analysis(true, pred, threshold):\n",
    "    tn, fp, fn, tp = confusion_matrix(true, get_classification(pred,threshold)).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    tpr = tp / (tp + fn)\n",
    "    f_score = 2 * precision * tpr / (precision + tpr)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print(\"Precision:\\t\\t\\t%1.2f\" % (precision))\n",
    "    print(\"Recall/TPR:\\t\\t\\t%1.2f\" % (recall))\n",
    "    print(\"f-score:\\t\\t\\t%1.2f\" % (f_score))\n",
    "    print(\"Accuracy:\\t\\t\\t%1.2f\" % (accuracy))\n",
    "c_m_analysis(Test_Y, predictions_linear, 0.5)\n",
    "\n",
    "# confusion matrix\n",
    "def get_classification(predictions,threshold):  #take value of prediction -> 0, 1\n",
    "    classes = np.zeros_like(predictions_linear)\n",
    "    for i in range(len(classes)):\n",
    "        if predictions[i] > threshold:\n",
    "            classes[i] = 1\n",
    "    return classes\n",
    "confusion_matrix(Test_Y, get_classification(predictions_linear, 0.5))\n",
    "\n",
    "# draw roc curve\n",
    "(fpr, tpr, thresholds) = roc_curve(Test_Y, predictions_linear)\n",
    "area = auc(fpr, tpr)\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, label = \"Out-Sample ROC Curve with area = %1.2f\" % area)\n",
    "plt.plot([0, 1], [0, 1], 'k')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = tree.DecisionTreeClassifier(max_depth = 10, criterion = 'entropy')\n",
    "tree_model.fit(Train_X_Tfidf, Train_Y,Train_Y)\n",
    "predictions_tree = tree_model.predict(Test_X_Tfidf) # prediction\n",
    "confusion_matrix(Test_Y, get_classification(predictions_tree, 0.5)) # confusion matrix\n",
    "c_m_analysis(Test_Y, predictions_tree, 0.5) # prediction performance\n",
    "# roc curve is as the same codes as above, the only difference is following:\n",
    "(fpr, tpr, thresholds1) = roc_curve(Test_Y, predictions_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf, Train_Y)\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf) # prediction\n",
    "confusion_matrix(Test_Y, get_classification(predictions_NB, 0.5)) # confusion matrix\n",
    "c_m_analysis(Test_Y, predictions_NB, 0.5) # prediction performance\n",
    "# roc curve is as the same codes as above, the only difference is following:\n",
    "(fpr, tpr, thresholds) = roc_curve(Test_Y, predictions_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C = 1.0, kernel = 'linear', degree = 3, gamma = 'auto')\n",
    "SVM.fit(Train_X_Tfidf, Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf) # prediction\n",
    "np.set_printoptions(threshold = np.inf)\n",
    "confusion_matrix(Test_Y,get_classification(predictions_SVM,0.5)) # confusion matrix\n",
    "c_m_analysis(Test_Y, predictions_SVM, 0.5) # prediction performance\n",
    "# roc curve is as the same codes as above, the only difference is following:\n",
    "(fpr, tpr, thresholds) = roc_curve(Test_Y, predictions_SVM)\n",
    "\n",
    "# New York restaurants prediction using SVM\n",
    "for i in range(0, len(predict_data['review'])):\n",
    "    predict_value = SVM.predict(Tfidf_vect.transform([str(predict_data['review'][i])]))\n",
    "    predict_data['predict_SVM'][i] = predict_value\n",
    "predict_data['predict_SVM'] = np.where((predict_data['predict_SVM'] > 0.5), 1, 0)\n",
    "\n",
    "sum=0\n",
    "for i in range(len(predict_data['review'])):\n",
    "    if int(predict_data['label'][i]) == int(predict_data['predict_SVM'][i]):\n",
    "        sum = sum + 1\n",
    "predict_accuracy = sum / len(predict_data['review']) # prediction accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
